{"cells":[{"cell_type":"markdown","metadata":{},"source":["This notebook is forked from [Konstantin Boyko](https://www.kaggle.com/konstantinboyko) and is based on ideas of [Med Ali Bouchhioua](https://www.kaggle.com/medali1992) and [Nischay Dhankhar](https://www.kaggle.com/nischaydnk). It implements a ResNet architecture followed by a GRU. This is then fed into the feed-forward classification head.\n","\n","My addition was the geometric pooling channel-independent layers. 1D CNNs build features using sliding kernels. Maybe the feature detects strange but important patterns, or maybe it detects common patterns that the model later wants to count. In the first example, max pooling seems appropriate, while for the second, average pooling seems appropriate. Geometric pooling allows each channel to learn an optimal pooling method."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:24.084764Z","iopub.status.busy":"2024-04-07T18:05:24.084421Z","iopub.status.idle":"2024-04-07T18:05:32.819570Z","shell.execute_reply":"2024-04-07T18:05:32.818283Z","shell.execute_reply.started":"2024-04-07T18:05:24.084730Z"},"papermill":{"duration":13.898864,"end_time":"2024-02-20T13:39:22.393405","exception":false,"start_time":"2024-02-20T13:39:08.494541","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import sys\n","import math\n","import time\n","import random\n","import datetime as dt\n","import numpy as np\n","import pandas as pd\n","\n","from glob import glob\n","from pathlib import Path\n","from typing import Dict, List, Union\n","import scipy.signal as scisig\n","from scipy.signal import butter, lfilter, freqz\n","from matplotlib import pyplot as plt\n","from tqdm.auto import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.optim.optimizer import Optimizer\n","from sklearn.model_selection import GroupKFold\n","\n","sys.path.append(\"/kaggle/input/kaggle-kl-div\")\n","import kaggle_kl_div\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","device = torch.device(\"cuda\")\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","\n","!cat /etc/os-release | grep -oP \"PRETTY_NAME=\\\"\\K([^\\\"]*)\"\n","print(f\"BUILD_DATE={os.environ['BUILD_DATE']}, CONTAINER_NAME={os.environ['CONTAINER_NAME']}\")\n","\n","try:\n","    print(\n","        f\"PyTorch Version:{torch.__version__}, CUDA is available:{torch.cuda.is_available()}, Version CUDA:{torch.version.cuda}\"\n","    )\n","    print(\n","        f\"Device Capability:{torch.cuda.get_device_capability()}, {torch.cuda.get_arch_list()}\"\n","    )\n","    print(\n","        f\"CuDNN Enabled:{torch.backends.cudnn.enabled}, Version:{torch.backends.cudnn.version()}\"\n","    )\n","except Exception:\n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["# Directory settings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:32.822518Z","iopub.status.busy":"2024-04-07T18:05:32.821708Z","iopub.status.idle":"2024-04-07T18:05:32.833897Z","shell.execute_reply":"2024-04-07T18:05:32.832914Z","shell.execute_reply.started":"2024-04-07T18:05:32.822482Z"},"trusted":true},"outputs":[],"source":["class APP:\n","    jupyter = \"ipykernel\" in globals()\n","    if not jupyter:\n","        try:\n","            if \"IPython\" in globals().get(\"__doc__\", \"\"):\n","                jupyter = True\n","        except Exception as inst:\n","            print(inst)\n","\n","    kaggle = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\") != \"\"\n","    local = os.environ.get(\"DOCKER_USING\", \"\") == \"LOCAL\"\n","    date_time_start = dt.datetime.now()\n","    dt_start_ymd_hms = date_time_start.strftime(\"%Y.%m.%d_%H-%M-%S\")\n","\n","    file_run_path = \"\"\n","    if jupyter:\n","        try:\n","            file_run_path = Path(globals().get(\"__vsc_ipynb_file__\", \"\"))\n","        except Exception as inst:\n","            print(inst)\n","\n","    else:\n","        try:\n","            file_run_path = Path(__file__)\n","        except Exception as inst:\n","            print(inst)\n","\n","    file_run_name = file_run_path.stem\n","    path_app = file_run_path.parent\n","    path_run = Path(os.getcwd())\n","    path_out = (\n","        Path(\"/kaggle/working\")\n","        if kaggle\n","        else file_run_path / f\"{file_run_name}_{dt_start_ymd_hms}\"\n","    )\n","\n","\n","OUTPUT_DIR = \"./\"\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","\n","print(f\"jupyter:{APP.jupyter}, kaggle:{APP.kaggle}, local:{APP.local}\")\n","print(APP.file_run_path)\n","print(APP.path_out)"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:32.835346Z","iopub.status.busy":"2024-04-07T18:05:32.835070Z","iopub.status.idle":"2024-04-07T18:05:32.861644Z","shell.execute_reply":"2024-04-07T18:05:32.860828Z","shell.execute_reply.started":"2024-04-07T18:05:32.835317Z"},"papermill":{"duration":0.029582,"end_time":"2024-02-20T13:39:22.435956","exception":false,"start_time":"2024-02-20T13:39:22.406374","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class CFG:\n","    VERSION = '90'\n","\n","    debug = False\n","    create_eegs = False\n","    apex = True\n","    visualize = False\n","    save_all_models = True\n","\n","    if debug:\n","        num_workers = 0\n","        parallel = False\n","    else:\n","        num_workers = os.cpu_count()\n","        parallel = True\n","\n","    model_name = \"resnet1d_gru\"\n","    optimizer = \"AdamW\"\n","\n","    factor = 0.9\n","    eps = 1e-6\n","    lr = 5e-3 # 8e-3\n","    min_lr = 5e-7 # 1e-6\n","\n","    batch_size = 64\n","    batch_koef_valid = 2\n","    batch_scheduler = True\n","    weight_decay = 1e-2\n","    gradient_accumulation_steps = 1\n","    max_grad_norm = 1e7\n","\n","    fixed_kernel_size = 5\n","    linear_layer_features = 304\n","    kernels = [3, 5, 7, 9, 11]\n","\n","    seq_length = 50  # seconds\n","    sampling_rate = 200  # Hz\n","    nsamples = seq_length * sampling_rate  # 10_000\n","    n_split_samples = 5\n","    out_samples = nsamples // n_split_samples  # 2_000\n","    sample_delta = nsamples - out_samples  # 8000\n","    sample_offset = sample_delta // 2\n","    multi_validation = False\n","\n","    train_by_stages = False\n","    train_by_folds = True\n","\n","    # 'GPD', 'GRDA', 'LPD', 'LRDA', 'Other', 'Seizure'\n","\n","    train_stages = [0, 1]\n","    epochs = [50, 100]\n","    test_total_eval = 4\n","    total_evaluators = [ \n","        [   \n","            {'band':(1, 3), 'excl_evals':[]}, \n","            {'band':(4, 4), 'excl_evals':['GPD']}, \n","            {'band':(5, 5), 'excl_evals':[]}, \n","        ], \n","        [   \n","            {'band':(6, 28), 'excl_evals':[]},\n","        ], \n","    ]        \n","    \n","    n_fold = 5\n","    train_folds = [0, 1, 2, 3, 4]\n","\n","    patience = 11\n","    seed = 2024\n","\n","    bandpass_filter = {\"low\": 0.5, \"high\": 20, \"order\": 2}\n","    rand_filter = {\"probab\": 0.1, \"low\": 10, \"high\": 20, \"band\": 1.0, \"order\": 2}\n","    freq_channels = []\n","    filter_order = 2\n","\n","    random_divide_signal = 0.05\n","    random_close_zone = 0.05\n","    random_common_negative_signal = 0.0\n","    random_common_reverse_signal = 0.0\n","    random_negative_signal = 0.05\n","    random_reverse_signal = 0.05\n","\n","    log_step = 100\n","    log_show = False\n","\n","    scheduler = \"CosineAnnealingLR\"\n","    cosanneal_params = {\n","        \"T_max\": 6,\n","        \"eta_min\": 1e-5,\n","        \"last_epoch\": -1,\n","    }\n","\n","    target_cols = [\n","        \"seizure_vote\",\n","        \"lpd_vote\",\n","        \"gpd_vote\",\n","        \"lrda_vote\",\n","        \"grda_vote\",\n","        \"other_vote\",\n","    ]\n","\n","    pred_cols = [x + \"_pred\" for x in target_cols]\n","\n","    map_features = [\n","        (\"Fp1\", \"T3\"),\n","        (\"T3\", \"O1\"),\n","        (\"Fp1\", \"C3\"),\n","        (\"C3\", \"O1\"),\n","        (\"Fp2\", \"C4\"),\n","        (\"C4\", \"O2\"),\n","        (\"Fp2\", \"T4\"),\n","        (\"T4\", \"O2\"),\n","    ]\n","\n","    eeg_features = [\"Fp1\", \"T3\", \"C3\", \"O1\", \"Fp2\", \"C4\", \"T4\", \"O2\"]\n","    feature_to_index = {x: y for x, y in zip(eeg_features, range(len(eeg_features)))}\n","    simple_features = []\n","    \n","    n_map_features = len(map_features)\n","    in_channels = n_map_features + n_map_features * len(freq_channels) + len(simple_features)\n","    target_size = len(target_cols)\n","\n","    path_inp = Path(\"/kaggle/input\")\n","    path_src = path_inp / \"hms-harmful-brain-activity-classification/\"\n","    file_train = path_src / \"train.csv\"\n","    path_train = path_src / \"train_eegs\"\n","    file_features_test = path_train / \"100261680.parquet\"\n","    file_eeg_specs = path_inp / \"eeg-spectrogram-by-lead-id-unique/eeg_specs.npy\"\n","    file_raw_eeg = path_inp / \"brain-eegs/eegs.npy\"\n","\n","    if APP.kaggle:\n","        num_workers = 2\n","        parallel = True\n","\n","print(CFG.feature_to_index)\n","print(CFG.eeg_features)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012027,"end_time":"2024-02-20T13:39:22.461399","exception":false,"start_time":"2024-02-20T13:39:22.449372","status":"completed"},"tags":[]},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:32.864639Z","iopub.status.busy":"2024-04-07T18:05:32.864018Z","iopub.status.idle":"2024-04-07T18:05:32.880617Z","shell.execute_reply":"2024-04-07T18:05:32.879779Z","shell.execute_reply.started":"2024-04-07T18:05:32.864605Z"},"papermill":{"duration":0.045586,"end_time":"2024-02-20T13:39:22.519244","exception":false,"start_time":"2024-02-20T13:39:22.473658","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n","    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n","\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=log_file)\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = init_logger()\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return \"%dm %ds\" % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n","\n","\n","def quantize_data(data, classes):\n","    mu_x = mu_law_encoding(data, classes)\n","    return mu_x  # quantized\n","\n","\n","def mu_law_encoding(data, mu):\n","    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n","    return mu_x\n","\n","\n","def mu_law_expansion(data, mu):\n","    s = np.sign(data) * (np.exp(np.abs(data) * np.log(mu + 1)) - 1) / mu\n","    return s\n","\n","\n","def butter_bandpass(lowcut, highcut, fs, order=5):\n","    return butter(order, [lowcut, highcut], fs=fs, btype=\"band\")\n","\n","\n","def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n","    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n","    y = lfilter(b, a, data)\n","    return y\n","\n","\n","def butter_lowpass_filter(\n","    data, cutoff_freq=20, sampling_rate=CFG.sampling_rate, order=4\n","):\n","    nyquist = 0.5 * sampling_rate\n","    normal_cutoff = cutoff_freq / nyquist\n","    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n","    filtered_data = lfilter(b, a, data, axis=0)\n","    return filtered_data\n","\n","\n","def denoise_filter(x):\n","    y = butter_bandpass_filter(x, CFG.lowcut, CFG.highcut, CFG.sampling_rate, order=6)\n","    y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n","    y = y[0:-1:4]\n","    return y"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011709,"end_time":"2024-02-20T13:40:46.512143","exception":false,"start_time":"2024-02-20T13:40:46.500434","status":"completed"},"tags":[]},"source":["# Parquet to EEG Signals Numpy Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:32.882332Z","iopub.status.busy":"2024-04-07T18:05:32.882074Z","iopub.status.idle":"2024-04-07T18:05:32.894464Z","shell.execute_reply":"2024-04-07T18:05:32.893709Z","shell.execute_reply.started":"2024-04-07T18:05:32.882310Z"},"papermill":{"duration":0.026463,"end_time":"2024-02-20T13:40:46.550473","exception":false,"start_time":"2024-02-20T13:40:46.52401","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def eeg_from_parquet(\n","    parquet_path: str, display: bool = False, seq_length=CFG.seq_length\n",") -> np.ndarray:\n","\n","    eeg = pd.read_parquet(parquet_path, columns=CFG.eeg_features)\n","    rows = len(eeg)\n","\n","    offset = (rows - CFG.nsamples) // 2\n","    eeg = eeg.iloc[offset : offset + CFG.nsamples]\n","\n","    if display:\n","        plt.figure(figsize=(10, 5))\n","        offset = 0\n","\n","\n","    data = np.zeros((CFG.nsamples, len(CFG.eeg_features)))\n","    for index, feature in enumerate(CFG.eeg_features):\n","        x = eeg[feature].values.astype(\"float32\")\n","\n","        mean = np.nanmean(x)\n","        nan_percentage = np.isnan(x).mean()  # percentage of NaN values in feature\n","\n","        if nan_percentage < 1:\n","            x = np.nan_to_num(x, nan=mean)\n","        else:\n","            x[:] = 0\n","        data[:, index] = x\n","\n","        if display:\n","            if index != 0:\n","                offset += x.max()\n","            plt.plot(range(CFG.nsamples), x - offset, label=feature)\n","            offset -= x.min()\n","\n","    if display:\n","        plt.legend()\n","        name = parquet_path.split(\"/\")[-1].split(\".\")[0]\n","        plt.yticks([])\n","        plt.title(f\"EEG {name}\", size=16)\n","        plt.show()\n","\n","    return data"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.014802,"end_time":"2024-02-20T13:42:17.914507","exception":false,"start_time":"2024-02-20T13:42:17.899705","status":"completed"},"tags":[]},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:32.895860Z","iopub.status.busy":"2024-04-07T18:05:32.895579Z","iopub.status.idle":"2024-04-07T18:05:32.927760Z","shell.execute_reply":"2024-04-07T18:05:32.926850Z","shell.execute_reply.started":"2024-04-07T18:05:32.895837Z"},"papermill":{"duration":0.0354,"end_time":"2024-02-20T13:42:17.964068","exception":false,"start_time":"2024-02-20T13:42:17.928668","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class EEGDataset(Dataset):\n","    def __init__(\n","        self,\n","        df: pd.DataFrame,\n","        batch_size: int,\n","        eegs: Dict[int, np.ndarray],\n","        mode: str = \"train\",\n","        downsample: int = None,\n","        bandpass_filter: Dict[str, Union[int, float]] = None,\n","        rand_filter: Dict[str, Union[int, float]] = None,\n","    ):\n","        self.df = df\n","        self.batch_size = batch_size\n","        self.mode = mode\n","        self.eegs = eegs\n","        self.downsample = downsample\n","        self.offset = None\n","        self.bandpass_filter = bandpass_filter\n","        self.rand_filter = rand_filter\n","        \n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        X, y_prob = self.__data_generation(index)\n","        if self.downsample is not None:\n","            X = X[:: self.downsample, :]\n","        output = {\n","            \"eeg\": torch.tensor(X, dtype=torch.float32),\n","            \"labels\": torch.tensor(y_prob, dtype=torch.float32),\n","        }\n","        return output\n","\n","    def set_offset(self, offset: int):\n","        self.offset = offset\n","\n","    def __data_generation(self, index):\n","        X = np.zeros(\n","            (CFG.out_samples, CFG.in_channels), dtype=\"float32\"\n","        )\n","\n","        random_divide_signal = False\n","        row = self.df.iloc[index]\n","        data = self.eegs[row.eeg_id]\n","        if CFG.nsamples != CFG.out_samples:\n","            if self.mode == \"train\":\n","                offset = (CFG.sample_delta * random.randint(0, 1000)) // 1000\n","            elif not self.offset is None:\n","                offset = self.offset\n","            else:\n","                offset = CFG.sample_offset\n","\n","            if self.mode == \"train\" and CFG.random_divide_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_divide_signal:\n","                random_divide_signal = True\n","                multipliers = [(1, 2), (2, 3), (3, 4), (3, 5)]\n","                koef_1, koef_2 = multipliers[random.randint(0, 3)]\n","                offset = (koef_1 * offset) // koef_2\n","                data = data[offset:offset+(CFG.out_samples * koef_2) // koef_1,:]\n","            else:\n","                data = data[offset:offset+CFG.out_samples,:]\n","\n","        reverse_signal = False\n","        negative_signal = False\n","        if self.mode == \"train\":\n","            if CFG.random_common_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_reverse_signal:\n","                reverse_signal = True\n","            if CFG.random_common_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_negative_signal:\n","                negative_signal = True\n","\n","        for i, (feat_a, feat_b) in enumerate(CFG.map_features):\n","            if self.mode == \"train\" and CFG.random_close_zone > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_close_zone:\n","                continue\n","            \n","            diff_feat = (\n","                data[:, CFG.feature_to_index[feat_a]]\n","                - data[:, CFG.feature_to_index[feat_b]]\n","            )\n","\n","            if self.mode == \"train\":\n","                if reverse_signal or CFG.random_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_reverse_signal:\n","                    diff_feat = np.flip(diff_feat)\n","                if negative_signal or CFG.random_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_negative_signal:\n","                    diff_feat = -diff_feat\n","\n","            if not self.bandpass_filter is None:\n","                diff_feat = butter_bandpass_filter(\n","                    diff_feat,\n","                    self.bandpass_filter[\"low\"],\n","                    self.bandpass_filter[\"high\"],\n","                    CFG.sampling_rate,\n","                    order=self.bandpass_filter[\"order\"],\n","                )\n","            \n","            if random_divide_signal:\n","                diff_feat = scisig.upfirdn([1.0, 1, 1.0], diff_feat, koef_1, koef_2)\n","                diff_feat = diff_feat[0:CFG.out_samples]\n","\n","            if (\n","                self.mode == \"train\"\n","                and not self.rand_filter is None\n","                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n","            ):\n","                lowcut = random.randint(\n","                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n","                )\n","                highcut = lowcut + self.rand_filter[\"band\"]\n","                diff_feat = butter_bandpass_filter(\n","                    diff_feat,\n","                    lowcut,\n","                    highcut,\n","                    CFG.sampling_rate,\n","                    order=self.rand_filter[\"order\"],\n","                )\n","\n","            X[:, i] = diff_feat\n","\n","        n = CFG.n_map_features\n","        if len(CFG.freq_channels) > 0:\n","            for i in range(CFG.n_map_features):\n","                diff_feat = X[:, i]\n","                for j, (lowcut, highcut) in enumerate(CFG.freq_channels):\n","                    band_feat = butter_bandpass_filter(\n","                        diff_feat, lowcut, highcut, CFG.sampling_rate, order=CFG.filter_order,  # 6\n","                    )\n","                    X[:, n] = band_feat\n","                    n += 1\n","\n","        for spml_feat in CFG.simple_features:\n","            feat_val = data[:, CFG.feature_to_index[spml_feat]]\n","            \n","            if not self.bandpass_filter is None:\n","                feat_val = butter_bandpass_filter(\n","                    feat_val,\n","                    self.bandpass_filter[\"low\"],\n","                    self.bandpass_filter[\"high\"],\n","                    CFG.sampling_rate,\n","                    order=self.bandpass_filter[\"order\"],\n","                )\n","\n","            if (\n","                self.mode == \"train\"\n","                and not self.rand_filter is None\n","                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n","            ):\n","                lowcut = random.randint(\n","                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n","                )\n","                highcut = lowcut + self.rand_filter[\"band\"]\n","                feat_val = butter_bandpass_filter(\n","                    feat_val,\n","                    lowcut,\n","                    highcut,\n","                    CFG.sampling_rate,\n","                    order=self.rand_filter[\"order\"],\n","                )\n","\n","            X[:, n] = feat_val\n","            n += 1\n","\n","        X = np.clip(X, -1024, 1024)\n","        X = np.nan_to_num(X, nan=0) / 32.0\n","        X = butter_lowpass_filter(X, order=CFG.filter_order)\n","\n","        y_prob = np.zeros(CFG.target_size, dtype=\"float32\")\n","        if self.mode != \"test\":\n","            y_prob = row[CFG.target_cols].values.astype(np.float32)\n","\n","        return X, y_prob"]},{"cell_type":"markdown","metadata":{},"source":["# Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:32.929571Z","iopub.status.busy":"2024-04-07T18:05:32.929185Z","iopub.status.idle":"2024-04-07T18:05:32.941574Z","shell.execute_reply":"2024-04-07T18:05:32.940716Z","shell.execute_reply.started":"2024-04-07T18:05:32.929539Z"},"trusted":true},"outputs":[],"source":["class KLDivLossWithLogits(nn.KLDivLoss):\n","    def __init__(self):\n","        super().__init__(reduction=\"batchmean\")\n","\n","    def forward(self, y, t):\n","        y = nn.functional.log_softmax(y, dim=1)\n","        loss = super().forward(y, t)\n","        return loss\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def seed_torch(seed):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031018,"end_time":"2024-02-20T13:42:28.75746","exception":false,"start_time":"2024-02-20T13:42:28.726442","status":"completed"},"tags":[]},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:14:01.196304Z","iopub.status.busy":"2024-04-07T18:14:01.195959Z","iopub.status.idle":"2024-04-07T18:14:01.231308Z","shell.execute_reply":"2024-04-07T18:14:01.230263Z","shell.execute_reply.started":"2024-04-07T18:14:01.196273Z"},"papermill":{"duration":0.05674,"end_time":"2024-02-20T13:42:28.844768","exception":false,"start_time":"2024-02-20T13:42:28.788028","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class ResNet_1D_Block(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size,\n","        stride,\n","        padding,\n","        downsampling,\n","        dilation=1,\n","        groups=1,\n","    ):\n","        super(ResNet_1D_Block, self).__init__()\n","\n","        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n","        self.relu_1 = nn.Hardswish()\n","        self.relu_2 = nn.Hardswish()\n","\n","        self.conv1 = nn.Conv1d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            dilation=dilation,\n","            groups=groups,\n","            bias=False,\n","        )\n","\n","        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n","        self.conv2 = nn.Conv1d(\n","            in_channels=out_channels,\n","            out_channels=out_channels,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            dilation=dilation,\n","            groups=groups,\n","            bias=False,\n","        )\n","\n","        self.maxpool = nn.MaxPool1d(\n","            kernel_size=2,\n","            stride=2,\n","            padding=0,\n","            dilation=dilation,\n","        )\n","        self.downsampling = downsampling\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.bn1(x)\n","        out = self.relu_1(out)\n","        out = self.conv1(out)\n","        out = self.bn2(out)\n","        out = self.relu_2(out)\n","        out = self.conv2(out)\n","\n","        out = self.maxpool(out)\n","        identity = self.downsampling(x)\n","\n","        out += identity\n","        return out\n","\n","class GeMPool(nn.Module):\n","    def __init__(self, width, ch=64, p=1, eps=1e-6, padding=2):\n","        super(GeMPool,self).__init__()\n","        self.p = nn.Parameter(torch.ones(1,ch,1)*p)\n","        self.eps = eps\n","        self.averager = nn.AvgPool1d(width, padding=padding)\n","        \n","    def gem(self, x, p, eps=1e-6):\n","        power = torch.pow(x.clamp(min=eps), p)\n","        avg = self.averager(power)\n","        root = torch.pow(avg, 1./p)\n","        return root\n","        \n","    def forward(self, x):\n","        return self.gem(x, self.p, eps=self.eps)\n","\n","class EEGNet(nn.Module):\n","    def __init__(\n","        self,\n","        kernels,\n","        in_channels,\n","        fixed_kernel_size,\n","        num_classes,\n","        linear_layer_features,\n","        groups=1,\n","    ):\n","        super(EEGNet, self).__init__()\n","        self.kernels = kernels\n","        self.planes = 24\n","        self.parallel_conv = nn.ModuleList()\n","        self.in_channels = in_channels\n","\n","        for kernel_size in list(self.kernels):\n","            sep_conv = nn.Conv1d(\n","                in_channels=in_channels,\n","                out_channels=self.planes,\n","                kernel_size=(kernel_size),\n","                stride=1,\n","                padding=0,\n","                groups=groups,\n","                bias=False,\n","            )\n","            self.parallel_conv.append(sep_conv)\n","\n","        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n","        self.relu_1 = nn.SiLU()\n","        self.relu_2 = nn.SiLU()\n","\n","        self.conv1 = nn.Conv1d(\n","            in_channels=self.planes,\n","            out_channels=self.planes,\n","            kernel_size=fixed_kernel_size,\n","            stride=2,\n","            padding=2,\n","            groups=groups,\n","            bias=False,\n","        )\n","\n","        self.block = self._make_resnet_layer(\n","            kernel_size=fixed_kernel_size,\n","            stride=1,\n","            groups=groups,\n","            padding=fixed_kernel_size // 2,\n","        )\n","        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n","        self.gempool = GeMPool(6, ch=self.planes, padding=2)\n","\n","        self.rnn = nn.GRU(\n","            input_size=self.in_channels,\n","            hidden_size=128,\n","            num_layers=1,\n","            bidirectional=True,\n","        )\n","\n","        self.fc = nn.Linear(in_features=linear_layer_features, out_features=num_classes)\n","\n","    def _make_resnet_layer(\n","        self,\n","        kernel_size,\n","        stride,\n","        groups=1,\n","        blocks=9,\n","        padding=0,\n","    ):\n","        layers = []\n","        for _ in range(blocks):\n","            downsampling = nn.Sequential(\n","                nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n","            )\n","            layers.append(\n","                ResNet_1D_Block(\n","                    in_channels=self.planes,\n","                    out_channels=self.planes,\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    downsampling=downsampling,\n","                    groups=groups,\n","                )\n","            )\n","        return nn.Sequential(*layers)\n","\n","    def extract_features(self, x):\n","        x = x.permute(0, 2, 1)\n","\n","        out_sep = []\n","        for i in range(len(self.kernels)):\n","            sep = self.parallel_conv[i](x)\n","            out_sep.append(sep)\n","\n","        out = torch.cat(out_sep, dim=2)\n","        out = self.bn1(out)\n","        out = self.relu_1(out)\n","        out = self.conv1(out)\n","\n","        out = self.block(out)\n","        out = self.bn2(out)\n","        out = self.relu_2(out)\n","        out = self.gempool(out)\n","\n","        out = out.reshape(out.shape[0], -1)\n","        rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n","        new_rnn_h = rnn_out[:, -1, :]\n","\n","        new_out = torch.cat([out, new_rnn_h], dim=1)\n","        return new_out\n","\n","    def forward(self, x):\n","        new_out = self.extract_features(x)\n","        result = self.fc(new_out)\n","        return result"]},{"cell_type":"markdown","metadata":{},"source":["# Train func"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.009204Z","iopub.status.busy":"2024-04-07T18:05:33.008908Z","iopub.status.idle":"2024-04-07T18:05:33.020850Z","shell.execute_reply":"2024-04-07T18:05:33.020082Z","shell.execute_reply.started":"2024-04-07T18:05:33.009181Z"},"trusted":true},"outputs":[],"source":["def train_fn(\n","    stage, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device\n","):\n","    model.train()\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","\n","    for step, batch in enumerate(train_loader):\n","        eegs = batch[\"eeg\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(eegs)\n","            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","\n","        losses.update(loss.item(), batch_size)\n","\n","        scaler.scale(loss).backward()\n","\n","        grad_norm = torch.nn.utils.clip_grad_norm_(\n","            model.parameters(), CFG.max_grad_norm\n","        )\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","\n","        if CFG.log_show and (\n","            step % CFG.log_step == 0 or step == (len(train_loader) - 1)\n","        ):\n","            # remain=timeSince(start, float(step + 1) / len(train_loader))\n","            LOGGER.info(\n","                f\"Epoch {epoch+1} [{step}/{len(train_loader)}] Loss: {losses.val:.4f} Loss Avg:{losses.avg:.4f}\"\n","            )\n","            # \"Elapsed {remain:s} Grad: {grad_norm:.4f}  LR: {cheduler.get_lr()[0]:.8f}\"\n","\n","    return losses.avg"]},{"cell_type":"markdown","metadata":{},"source":["# Valid Func"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.022282Z","iopub.status.busy":"2024-04-07T18:05:33.022004Z","iopub.status.idle":"2024-04-07T18:05:33.034610Z","shell.execute_reply":"2024-04-07T18:05:33.033795Z","shell.execute_reply.started":"2024-04-07T18:05:33.022260Z"},"trusted":true},"outputs":[],"source":["def valid_fn(stage, epoch, valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    targets = []\n","    start = end = time.time()\n","\n","    for step, batch in enumerate(valid_loader):\n","        eegs = batch[\"eeg\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        batch_size = labels.size(0)\n","\n","        with torch.no_grad():\n","            y_preds = model(eegs)\n","            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","\n","        losses.update(loss.item(), batch_size)\n","        preds.append(nn.Softmax(dim=1)(y_preds).to(\"cpu\").numpy())\n","        targets.append(labels.to(\"cpu\").numpy())\n","        end = time.time()\n","\n","        if CFG.log_show and (\n","            step % CFG.log_step == 0 or step == (len(valid_loader) - 1)\n","        ):\n","            # remain=timeSince(start, float(step + 1) / len(valid_loader))\n","            LOGGER.info(\n","                f\"Epoch {epoch+1} VALIDATION: [{step}/{len(valid_loader)}] Val Loss: {losses.val:.4f} Val Loss Avg: {losses.avg:.4f}\"\n","            )\n","            # Elapsed {remain:s}\n","\n","    predictions = np.concatenate(preds)\n","    targets = np.concatenate(targets)\n","\n","    return losses.avg, predictions"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031254,"end_time":"2024-02-20T13:42:29.808216","exception":false,"start_time":"2024-02-20T13:42:29.776962","status":"completed"},"tags":[]},"source":["# Optimizer & Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.036330Z","iopub.status.busy":"2024-04-07T18:05:33.036005Z","iopub.status.idle":"2024-04-07T18:05:33.046852Z","shell.execute_reply":"2024-04-07T18:05:33.045945Z","shell.execute_reply.started":"2024-04-07T18:05:33.036300Z"},"trusted":true},"outputs":[],"source":["def build_optimizer(cfg, model):\n","    return AdamW(model.parameters(), lr=cfg.lr, weight_decay=CFG.weight_decay)\n","\n","def get_scheduler(optimizer):\n","    return CosineAnnealingLR(optimizer, **CFG.cosanneal_params)"]},{"cell_type":"markdown","metadata":{},"source":["# Train Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.062548Z","iopub.status.busy":"2024-04-07T18:05:33.062057Z","iopub.status.idle":"2024-04-07T18:05:33.083559Z","shell.execute_reply":"2024-04-07T18:05:33.082678Z","shell.execute_reply.started":"2024-04-07T18:05:33.062523Z"},"papermill":{"duration":0.05868,"end_time":"2024-02-20T13:42:29.897814","exception":false,"start_time":"2024-02-20T13:42:29.839134","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def train_loop(stage, epochs, folds, fold, directory, prev_dir, eggs):\n","    train_folds = folds[folds[\"fold\"] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds[\"fold\"] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = EEGDataset(\n","        train_folds,\n","        batch_size=CFG.batch_size,\n","        mode=\"train\",\n","        eegs=eggs,\n","        bandpass_filter=CFG.bandpass_filter,\n","        rand_filter=CFG.rand_filter,\n","    )\n","        \n","    valid_dataset = EEGDataset(\n","        valid_folds,\n","        batch_size=CFG.batch_size,\n","        mode=\"valid\",\n","        eegs=eggs,\n","        bandpass_filter=CFG.bandpass_filter,\n","    )\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=True,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        batch_size=CFG.batch_size * CFG.batch_koef_valid,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    LOGGER.info(\n","        f\"========== stage: {stage} fold: {fold} training {len(train_loader)} / {len(valid_loader)} ==========\"\n","    )\n","\n","    model = EEGNet(\n","        kernels=CFG.kernels,\n","        in_channels=CFG.in_channels,\n","        fixed_kernel_size=CFG.fixed_kernel_size,\n","        num_classes=CFG.target_size,\n","        linear_layer_features=CFG.linear_layer_features,\n","    )\n","\n","    if stage > 1:\n","        model_weight = f\"{prev_dir}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage-1}_fold-{fold}_best.pth\"\n","        checkpoint = torch.load(model_weight, map_location=device)\n","        model.load_state_dict(checkpoint[\"model\"])\n","\n","    model.to(device)\n","\n","    # CPMP: wrap the model to use all GPUs\n","    if CFG.parallel:\n","        model = nn.DataParallel(model)\n","\n","    optimizer = build_optimizer(CFG, model)\n","    scheduler = get_scheduler(optimizer)\n","    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n","\n","    best_score = np.inf\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(\n","            stage,\n","            fold,\n","            train_loader,\n","            model,\n","            criterion,\n","            optimizer,\n","            epoch,\n","            scheduler,\n","            device,\n","        )\n","\n","        # eval\n","        valid_dataset.set_offset(CFG.sample_offset)\n","        avg_val_loss, predictions = valid_fn(\n","            stage,\n","            epoch,\n","            valid_loader,\n","            model,\n","            criterion,\n","            device,\n","        )\n","        \n","        avg_loss_line = ''\n","        if CFG.multi_validation:\n","            multi_avg_val_loss = np.zeros(CFG.n_split_samples)\n","            start = (2 * CFG.sample_delta) // CFG.n_split_samples\n","            finish = (3 * CFG.sample_delta) // CFG.n_split_samples\n","            delta = (finish - start) // 5\n","            for i in range(CFG.n_split_samples):\n","                valid_dataset.set_offset(start)\n","                multi_avg_val_loss[i], _ = valid_fn(\n","                    stage,\n","                    epoch,\n","                    valid_loader,\n","                    model,\n","                    criterion,\n","                    device,\n","                )\n","                avg_loss_line += f\" {multi_avg_val_loss[i]:.4f}\"\n","                start += delta\n","            avg_loss_line += f\" mean={np.mean(multi_avg_val_loss):.4f}\"\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(\n","            f\"Epoch {epoch+1} Avg Train Loss: {avg_loss:.4f} Avg Valid Loss: {avg_val_loss:.4f} / {avg_loss_line}\"\n","        )\n","\n","        if CFG.save_all_models:\n","            torch.save(\n","                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n","                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_epoch-{epoch}_val-{avg_val_loss:.4f}_train-{avg_loss:.4f}.pth\",\n","            )\n","\n","        if best_score > avg_val_loss:\n","            best_score = avg_val_loss\n","            LOGGER.info(f\"Epoch {epoch+1} Save Best Valid Loss: {avg_val_loss:.4f}\")\n","            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n","            torch.save(\n","                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n","                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n","            )\n","\n","    predictions = torch.load(\n","        f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n","        map_location=torch.device(\"cpu\"),\n","    )[\"predictions\"]\n","\n","    # valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","    valid_folds[CFG.pred_cols] = predictions\n","    valid_folds[CFG.target_cols] = valid_labels\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds, best_score"]},{"cell_type":"markdown","metadata":{},"source":["# Load train data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.085001Z","iopub.status.busy":"2024-04-07T18:05:33.084692Z","iopub.status.idle":"2024-04-07T18:05:33.566000Z","shell.execute_reply":"2024-04-07T18:05:33.564919Z","shell.execute_reply.started":"2024-04-07T18:05:33.084977Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(CFG.file_train)\n","TARGETS = train.columns[-6:]\n","print(\"Train shape:\", train.shape)\n","print(\"Targets\", list(TARGETS))\n","\n","train[\"total_evaluators\"] = train[CFG.target_cols].sum(axis=1)\n","\n","train_uniq = train.drop_duplicates(subset=[\"eeg_id\"] + list(TARGETS))\n","\n","print(f\"There are {train.patient_id.nunique()} patients in the training data.\")\n","print(f\"There are {train.eeg_id.nunique()} EEG IDs in the training data.\")\n","print(f\"There are {train_uniq.shape[0]} unique eeg_id + votes in the training data.\")\n","\n","if CFG.visualize:\n","    train_uniq.eeg_id.value_counts().value_counts().plot(\n","        kind=\"bar\",\n","        title=f\"Distribution of Count of EEG w Unique Vote: \"\n","        f\"{train_uniq.shape[0]} examples\",\n","    )\n","\n","del train_uniq\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.567537Z","iopub.status.busy":"2024-04-07T18:05:33.567237Z","iopub.status.idle":"2024-04-07T18:05:33.986178Z","shell.execute_reply":"2024-04-07T18:05:33.985103Z","shell.execute_reply.started":"2024-04-07T18:05:33.567510Z"},"trusted":true},"outputs":[],"source":["if CFG.visualize:\n","    plt.figure(figsize=(10, 6))\n","    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n","    plt.title(\"Histogram of Total Evaluators\")\n","    plt.xlabel(\"Total Evaluators\")\n","    plt.ylabel(\"Frequency\")\n","    plt.grid(True)\n","    plt.show()\n","\n","tst_eeg_df = pd.read_parquet(CFG.file_features_test)\n","tst_eeg_features = tst_eeg_df.columns\n","print(f\"There are {len(tst_eeg_features)} raw eeg features\")\n","print(list(tst_eeg_features))\n","del tst_eeg_df\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Split Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:05:33.988307Z","iopub.status.busy":"2024-04-07T18:05:33.987907Z","iopub.status.idle":"2024-04-07T18:07:04.990077Z","shell.execute_reply":"2024-04-07T18:07:04.989196Z","shell.execute_reply.started":"2024-04-07T18:05:33.988270Z"},"trusted":true},"outputs":[],"source":["# %%time\n","all_eeg_specs = np.load(CFG.file_eeg_specs, allow_pickle=True).item()\n","\n","train = train[train[\"label_id\"].isin(all_eeg_specs.keys())].copy()\n","print(train.shape[0])\n","\n","y_data = train[TARGETS].values + 0.166666667  # Regularization value\n","y_data = y_data / y_data.sum(axis=1, keepdims=True)\n","train[TARGETS] = y_data\n","\n","train[\"target\"] = train[\"expert_consensus\"]\n","\n","train[train['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:07:04.991640Z","iopub.status.busy":"2024-04-07T18:07:04.991346Z","iopub.status.idle":"2024-04-07T18:07:05.044331Z","shell.execute_reply":"2024-04-07T18:07:05.043386Z","shell.execute_reply.started":"2024-04-07T18:07:04.991616Z"},"trusted":true},"outputs":[],"source":["if CFG.test_total_eval > 0:\n","    train['key_id'] = range(train.shape[0])\n","\n","    train_pop_olds = []\n","    for total_eval in CFG.total_evals_old:\n","        if type(total_eval) is list:\n","            pop_idx = (train[\"total_evaluators\"] >= total_eval[0][0]) & (\n","                train[\"total_evaluators\"] < total_eval[0][1]\n","            ) | (train[\"total_evaluators\"] >= total_eval[1][0]) & (\n","                train[\"total_evaluators\"] < total_eval[1][1]\n","            )\n","        else:\n","            pop_idx = (train[\"total_evaluators\"] >= total_eval[0]) & (\n","                train[\"total_evaluators\"] < total_eval[1]\n","            )\n","\n","        train_pop = train[pop_idx].copy().reset_index()\n","\n","        sgkf = GroupKFold(n_splits=CFG.n_fold)\n","        train_pop[\"fold\"] = -1\n","        for fold_id, (_, val_idx) in enumerate(\n","            sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n","        ):\n","            train_pop.loc[val_idx, \"fold\"] = fold_id\n","\n","        train_pop_olds.append(train_pop)\n","        print(train_pop.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:07:05.045818Z","iopub.status.busy":"2024-04-07T18:07:05.045510Z","iopub.status.idle":"2024-04-07T18:07:05.118869Z","shell.execute_reply":"2024-04-07T18:07:05.117823Z","shell.execute_reply.started":"2024-04-07T18:07:05.045765Z"},"trusted":true},"outputs":[],"source":["train_pops = []\n","for eval_list in CFG.total_evaluators:\n","    result=[]\n","    train_pop = train  \n","    for eval_dict in eval_list:\n","        band = eval_dict['band']\n","        pop_idx = (train_pop[\"total_evaluators\"] >= band[0]) \n","        pop_idx &= (train_pop[\"total_evaluators\"] <= band[1])\n","        for exclude in eval_dict['excl_evals']:\n","            pop_idx &= ~(train_pop['expert_consensus'] == exclude)\n","            pass\n","        result.append(train_pop[pop_idx])\n","    train_pop = pd.concat(result).copy().reset_index()\n","\n","    sgkf = GroupKFold(n_splits=CFG.n_fold)\n","    train_pop[\"fold\"] = -1\n","    for fold_id, (_, val_idx) in enumerate(\n","        sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n","    ):\n","        train_pop.loc[val_idx, \"fold\"] = fold_id\n","\n","    train_pops.append(train_pop)\n","    print(train_pop.shape[0])\n","\n","train_0 = train_pops[0]\n","train_0[train_0['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:07:05.120954Z","iopub.status.busy":"2024-04-07T18:07:05.120296Z","iopub.status.idle":"2024-04-07T18:07:05.401653Z","shell.execute_reply":"2024-04-07T18:07:05.400670Z","shell.execute_reply.started":"2024-04-07T18:07:05.120921Z"},"trusted":true},"outputs":[],"source":["if CFG.test_total_eval > 0:\n","    df_old = train_pop_olds[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n","    df_new = train_pops[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n","\n","    #outer merge the two DataFrames, adding an indicator column called 'Exist'\n","    diff_df = pd.merge(df_old, df_new, how='outer', indicator='Exist')\n","\n","    #find which rows don't exist in both DataFrames\n","    diff_df = diff_df.loc[diff_df['Exist'] != 'both']\n","    display(diff_df)\n","\n","    del df_old, df_new, diff_df, train_pop_olds\n","    _ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:07:05.406031Z","iopub.status.busy":"2024-04-07T18:07:05.405694Z","iopub.status.idle":"2024-04-07T18:07:05.550147Z","shell.execute_reply":"2024-04-07T18:07:05.549031Z","shell.execute_reply.started":"2024-04-07T18:07:05.406003Z"},"trusted":true},"outputs":[],"source":["if CFG.visualize:\n","    print(\"Pop 1: train unique eeg_id + votes shape:\", train_pops[0].shape)\n","    plt.figure(figsize=(10, 6))\n","    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n","    plt.title(\"Histogram of Total Evaluators\")\n","    plt.xlabel(\"Total Evaluators\")\n","    plt.ylabel(\"Frequency\")\n","    plt.grid(True)\n","    plt.show()\n","\n","del all_eeg_specs\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Deduplicate Train EEG Id"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:07:05.552098Z","iopub.status.busy":"2024-04-07T18:07:05.551674Z","iopub.status.idle":"2024-04-07T18:08:58.711391Z","shell.execute_reply":"2024-04-07T18:08:58.710527Z","shell.execute_reply.started":"2024-04-07T18:07:05.552063Z"},"trusted":true},"outputs":[],"source":["# %%time\n","if CFG.create_eegs:\n","    all_eegs = {}\n","    visualize = 1 if CFG.visualize else 0\n","    eeg_ids = train.eeg_id.unique()\n","\n","    for i, eeg_id in tqdm(enumerate(eeg_ids)):\n","\n","        # Сохранить ЭЭГ в словаре Python для массивов numpy\n","        eeg_path = CFG.path_train / f\"{eeg_id}.parquet\"\n","\n","        # Вырезаем среднюю 50 секундную часть и заполняем по среднему Nan\n","        data = eeg_from_parquet(eeg_path, display=i < visualize)\n","        all_eegs[eeg_id] = data\n","\n","        if i == visualize:\n","            if CFG.create_eegs:\n","                print(\n","                    f\"Processing {train['eeg_id'].nunique()} eeg parquets... \", end=\"\"\n","                )\n","            else:\n","                print(f\"Reading {len(eeg_ids)} eeg NumPys from disk.\")\n","                break\n","    np.save(\"./eegs\", all_eegs)\n","\n","else:\n","    all_eegs = np.load(CFG.file_raw_eeg, allow_pickle=True).item()\n","\n","if CFG.visualize:\n","    frequencies = [1, 2, 4, 8, 16][::-1]  # frequencies in Hz\n","    x = [all_eegs[eeg_ids[0]][:, 0]]  # select one EEG feature\n","\n","    for frequency in frequencies:\n","        x.append(butter_lowpass_filter(x[0], cutoff_freq=frequency))\n","\n","    plt.figure(figsize=(12, 8))\n","    plt.plot(range(CFG.nsamples), x[0], label=\"without filter\")\n","    for k in range(1, len(x)):\n","        plt.plot(\n","            range(CFG.nsamples),\n","            x[k] - k * (x[0].max() - x[0].min()),\n","            label=f\"with filter {frequencies[k-1]}Hz\",\n","        )\n","\n","    plt.legend()\n","    plt.yticks([])\n","    plt.title(\"Butter Low-Pass Filter Examples\", size=18)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:08:58.712815Z","iopub.status.busy":"2024-04-07T18:08:58.712525Z","iopub.status.idle":"2024-04-07T18:08:58.724416Z","shell.execute_reply":"2024-04-07T18:08:58.723537Z","shell.execute_reply.started":"2024-04-07T18:08:58.712775Z"},"trusted":true},"outputs":[],"source":["if CFG.visualize:\n","    train_dataset = EEGDataset(\n","        train_pops[0], batch_size=CFG.batch_size, eegs=all_eegs, mode=\"train\"\n","    )\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    output = train_dataset[0]\n","    X, y = output[\"eeg\"], output[\"labels\"]\n","    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n","\n","    iot = torch.randn(2, CFG.nsamples, CFG.in_channels)  # .cuda()\n","    model = EEGNet(\n","        kernels=CFG.kernels,\n","        in_channels=CFG.in_channels,\n","        fixed_kernel_size=CFG.fixed_kernel_size,\n","        num_classes=CFG.target_size,\n","        linear_layer_features=CFG.linear_layer_features,\n","    )\n","    output = model(iot)\n","    print(output.shape)\n","\n","    for batch in train_loader:\n","        X = batch.pop(\"eeg\")\n","        y = batch.pop(\"labels\")\n","        for item in range(4):\n","            plt.figure(figsize=(20, 4))\n","            offset = 0\n","            for col in range(X.shape[-1]):\n","                if col != 0:\n","                    offset -= X[item, :, col].min()\n","                plt.plot(\n","                    range(CFG.nsamples),\n","                    X[item, :, col] + offset,\n","                    label=f\"feature {col+1}\",\n","                )\n","                offset += X[item, :, col].max()\n","            tt = f\"{y[col][0]:0.1f}\"\n","            for t in y[col][1:]:\n","                tt += f\", {t:0.1f}\"\n","            plt.title(f\"EEG_Id = {eeg_ids[item]}\\nTarget = {tt}\", size=14)\n","            plt.legend()\n","            plt.show()\n","        break\n","\n","    del iot, model\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Train Stages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:08:58.725910Z","iopub.status.busy":"2024-04-07T18:08:58.725598Z","iopub.status.idle":"2024-04-07T18:08:58.739688Z","shell.execute_reply":"2024-04-07T18:08:58.738821Z","shell.execute_reply.started":"2024-04-07T18:08:58.725885Z"},"trusted":true},"outputs":[],"source":["def get_score(preds, targets):\n","    oof = pd.DataFrame(preds.copy())\n","    oof[\"id\"] = np.arange(len(oof))\n","    true = pd.DataFrame(targets.copy())\n","    true[\"id\"] = np.arange(len(true))\n","    cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n","    return cv\n","\n","\n","def get_result(result_df):\n","    gt = result_df[[\"eeg_id\"] + CFG.target_cols]\n","    gt.sort_values(by=\"eeg_id\", inplace=True)\n","    gt.reset_index(inplace=True, drop=True)\n","    preds = result_df[[\"eeg_id\"] + CFG.pred_cols]\n","    preds.columns = [\"eeg_id\"] + CFG.target_cols\n","    preds.sort_values(by=\"eeg_id\", inplace=True)\n","    preds.reset_index(inplace=True, drop=True)\n","    score_loss = get_score(gt[CFG.target_cols], preds[CFG.target_cols])\n","    LOGGER.info(f\"Score with best loss weights: {score_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:08:58.741134Z","iopub.status.busy":"2024-04-07T18:08:58.740868Z","iopub.status.idle":"2024-04-07T18:08:58.754345Z","shell.execute_reply":"2024-04-07T18:08:58.753470Z","shell.execute_reply.started":"2024-04-07T18:08:58.741112Z"},"papermill":{"duration":2650.233098,"end_time":"2024-02-20T14:26:40.161837","exception":false,"start_time":"2024-02-20T13:42:29.928739","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if __name__ == \"__main__\" and CFG.train_by_stages:\n","    seed_torch(seed=CFG.seed)\n","\n","    prev_dir = \"\"\n","    for stage in range(len(CFG.total_evaluators)):\n","        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n","        if not os.path.exists(pop_dir):\n","            os.makedirs(pop_dir)\n","\n","        if stage not in CFG.train_stages:\n","            prev_dir = pop_dir\n","            continue\n","\n","        oof_df = pd.DataFrame()\n","        scores = []\n","        for fold in CFG.train_folds:\n","            train_oof_df, score = train_loop(\n","                stage=stage + 1,\n","                epochs=CFG.epochs[stage],\n","                fold=fold,\n","                folds=train_pops[stage],\n","                directory=pop_dir,\n","                prev_dir=prev_dir,\n","                eggs=all_eegs,\n","            )\n","\n","            oof_df = pd.concat([oof_df, train_oof_df])\n","            scores.append(score)\n","\n","            LOGGER.info(f\"========== stage: {stage+1} fold: {fold} result ==========\")\n","            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n","\n","        LOGGER.info(f\"==================== CV ====================\")\n","        LOGGER.info(f\"Score with best loss weights: {np.mean(scores):.4f}\")\n","\n","        oof_df.reset_index(drop=True, inplace=True)\n","        oof_df.to_csv(\n","            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n","            index=False,\n","        )\n","\n","        prev_dir = pop_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T18:14:18.523280Z","iopub.status.busy":"2024-04-07T18:14:18.522839Z","iopub.status.idle":"2024-04-07T18:15:17.669621Z","shell.execute_reply":"2024-04-07T18:15:17.668079Z","shell.execute_reply.started":"2024-04-07T18:14:18.523245Z"},"trusted":true},"outputs":[],"source":["if __name__ == \"__main__\" and CFG.train_by_folds:\n","    seed_torch(seed=CFG.seed)\n","\n","    stages_scores = {i: [] for i in CFG.train_stages}\n","    stages_oof_df = {i: pd.DataFrame() for i in CFG.train_stages}\n","\n","    for fold in CFG.train_folds:\n","\n","        prev_dir = \"\"\n","        for stage in range(len(CFG.total_evaluators)):\n","\n","            pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n","            if not os.path.exists(pop_dir):\n","                os.makedirs(pop_dir)\n","\n","            if stage not in CFG.train_stages:\n","                prev_dir = pop_dir\n","                continue\n","\n","            train_oof_df, score = train_loop(\n","                stage=stage + 1,\n","                epochs=CFG.epochs[stage],\n","                fold=fold,\n","                folds=train_pops[stage],\n","                directory=pop_dir,\n","                prev_dir=prev_dir,\n","                eggs=all_eegs,\n","            )\n","\n","            stages_oof_df[stage] = pd.concat([stages_oof_df[stage], train_oof_df])\n","            stages_scores[stage].append(score)\n","\n","            prev_dir = pop_dir\n","\n","            LOGGER.info(f\"========== fold: {fold} stage: {stage+1} result ==========\")\n","            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n","\n","    for stage, scores in stages_scores.items():\n","        LOGGER.info(f\"============ CV score with best loss weights ============\")\n","        LOGGER.info(f\"Stage {stage}: {np.mean(scores):.4f}\")\n","\n","    for stage, oof_df in stages_oof_df.items():\n","        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n","        oof_df.reset_index(drop=True, inplace=True)\n","        oof_df.to_csv(\n","            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n","            index=False,\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-07T18:09:06.567757Z","iopub.status.idle":"2024-04-07T18:09:06.568146Z","shell.execute_reply":"2024-04-07T18:09:06.567992Z","shell.execute_reply.started":"2024-04-07T18:09:06.567976Z"},"papermill":{"duration":0.167828,"end_time":"2024-02-20T15:08:05.860071","exception":false,"start_time":"2024-02-20T15:08:05.692243","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# === Pre-process OOF ===\n","gt = oof_df[[\"eeg_id\"] + CFG.target_cols]\n","gt.sort_values(by=\"eeg_id\", inplace=True)\n","gt.reset_index(inplace=True, drop=True)\n","\n","preds = oof_df[[\"eeg_id\"] + CFG.pred_cols]\n","preds.columns = [\"eeg_id\"] + CFG.target_cols\n","preds.sort_values(by=\"eeg_id\", inplace=True)\n","preds.reset_index(inplace=True, drop=True)\n","\n","y_trues = gt[CFG.target_cols]\n","y_preds = preds[CFG.target_cols]\n","\n","oof = pd.DataFrame(y_preds.copy())\n","oof[\"id\"] = np.arange(len(oof))\n","\n","true = pd.DataFrame(y_trues.copy())\n","true[\"id\"] = np.arange(len(true))\n","\n","cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n","print(f\"CV Score with resnet1D_gru Raw EEG = {cv:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7469972,"sourceId":59093,"sourceType":"competition"},{"datasetId":4297749,"sourceId":7392733,"sourceType":"datasetVersion"},{"datasetId":4317718,"sourceId":7465251,"sourceType":"datasetVersion"},{"datasetId":4378712,"sourceId":7517324,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":5345.179582,"end_time":"2024-02-20T15:08:09.804733","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-20T13:39:04.625151","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"088fc3f5e4264683bbeaa2d929fab599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c3d333449fb412dbc86ca31835ef4b0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ba5491929cf4cb1bbb8e28514933aa6","value":1}},"11bfd3544cf8411e96cd7ede1451518c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5246ec5881c147f09c90846e9911dda6","placeholder":"​","style":"IPY_MODEL_db8fdfb810194e03b66c7c5e5077ccf5","value":" 1/? [00:00&lt;00:00,  1.37it/s]"}},"2c3d333449fb412dbc86ca31835ef4b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3ba5491929cf4cb1bbb8e28514933aa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5246ec5881c147f09c90846e9911dda6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58eb07f979fd49e8a9155ad60c77923c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993085d825a542b1b5c44cd42f7f8c89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c818523154ec45689615162b4c03bf96","IPY_MODEL_088fc3f5e4264683bbeaa2d929fab599","IPY_MODEL_11bfd3544cf8411e96cd7ede1451518c"],"layout":"IPY_MODEL_fa70a85db4944741a7d7b13cef19bb70"}},"c818523154ec45689615162b4c03bf96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58eb07f979fd49e8a9155ad60c77923c","placeholder":"​","style":"IPY_MODEL_ea4dcc5f4fcf4011a7ba7dc587109a9b","value":""}},"db8fdfb810194e03b66c7c5e5077ccf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea4dcc5f4fcf4011a7ba7dc587109a9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa70a85db4944741a7d7b13cef19bb70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
